{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azaansherani/Projects/blob/main/ResumeParser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC5h3rr5ECQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5ef271-e45a-49b0-ce30-9a09410a5c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (20201018)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (2.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (4.0.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (37.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdfx in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: pdfminer.six==20201018 in /usr/local/lib/python3.7/dist-packages (from pdfx) (20201018)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfx) (4.0.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20201018->pdfx) (2.4.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20201018->pdfx) (37.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six==20201018->pdfx) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20201018->pdfx) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer.six\n",
        "!pip install pdfx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukvbECYl6v_N"
      },
      "outputs": [],
      "source": [
        "# name of resume pdf file\n",
        "resume = \"/content/Zeeshan Islam CV (1).pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5NqIwzVEDdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d958b072-4108-4cb6-f5fa-a3f541056800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D A T A   A N A L Y S T  \n",
            "\n",
            "Z E E S H A N\n",
            "I S L A M\n",
            "\n",
            "GET IN CONTACT\n",
            "\n",
            "Mobile: 9830450347\n",
            "\n",
            "linkedin.com/in/islamzeeshan/\n",
            "hackerrank.com/zeeshanislam1511\n",
            "\n",
            "E-Mail:\n",
            "zeeshanislam1511998@gmail.com\n",
            "zees17is@cmrit.ac.in\n",
            "\n",
            "WORK EXPERIENCE\n",
            " \n",
            "Data Analyst Intern\n",
            "InsAnalytics, Kolkata | Jan’20 to March’20\n",
            "Cleaned and optimized the data.\n",
            "Built attractive data visualization dashboards using Tableau and Excel.\n",
            "Built Linear Regression and Classification models using Python. \n",
            "\n",
            "Marketing and Content Management \n",
            "Techstars Startup Weekend | Sep'18 to Nov’19 \n",
            "Create Social Media Content for Facebook, Instagram, Twitter and LinkedIn.\n",
            "Communicate with design and partnership teams to keep everything in order.\n",
            "Communicate and interact with VCs, mentors, founders.\n",
            "Communicate and interact with participants.\n",
            "MC for the events.\n",
            "Network and build connections.\n",
            " \n",
            "SEO FRIENDLY CONTENT CREATOR\n",
            "Digital Edge | September’2016 to October’18 \n",
            "Create unique, attractive and SEO friendly content for E-Commerce Websites and\n",
            "online blogs.\n",
            "Research and analyse the necessary topics.\n",
            "\n",
            "ONLINE TUTOR(English)\n",
            "Sling | September’2016 to November’2016\n",
            "Increase the spoken English proficiency of our app users through text and voice\n",
            "messages.\n",
            "\n",
            "POSITION OF LEADERSHIP\n",
            "\n",
            "• Literary Club Head | 2019-2020\n",
            "• Marketing and Sponsorship team Lead - Hackiton 3.0\n",
            "• Event Co-ordinator for Cultura'19\n",
            "\n",
            "CERTIFICATIONS\n",
            "Python Specialization: Coursera\n",
            "\n",
            "Mathematics for Machine Learning\n",
            "Specialization: Coursera\n",
            "\n",
            "Machine Learning by Andrew Ng:\n",
            "Coursera\n",
            "\n",
            "TECHNICAL SKILLS\n",
            "\n",
            "Tableau\n",
            " Python\n",
            " R\n",
            " Java\n",
            " MS-Excel\n",
            " Machine Learning\n",
            " Business Statistics\n",
            " SQLlite\n",
            "\n",
            "EDUCATION\n",
            "\n",
            "· BE\n",
            "CMR Institute of Technology\n",
            "Stream-CSE\n",
            "CGPA-7.47\n",
            " \n",
            "· XII\n",
            "AKLANK PUBLIC SCHOOL\n",
            "CBSE- 79%\n",
            " \n",
            "· X\n",
            "DON BOSCO, PARK\n",
            "CIRCUS\n",
            "ICSE-89%\n",
            "\n",
            "\f\n",
            "['D A T A   A N A L Y S T  ', '', 'Z E E S H A N', 'I S L A M', '', 'GET IN CONTACT', '', 'Mobile: 9830450347', '', 'linkedin.com/in/islamzeeshan/', 'hackerrank.com/zeeshanislam1511', '', 'E-Mail:', 'zeeshanislam1511998@gmail.com', 'zees17is@cmrit.ac.in', '', 'WORK EXPERIENCE', ' ', 'Data Analyst Intern', 'InsAnalytics, Kolkata | Jan’20 to March’20', 'Cleaned and optimized the data.', 'Built attractive data visualization dashboards using Tableau and Excel.', 'Built Linear Regression and Classification models using Python. ', '', 'Marketing and Content Management ', \"Techstars Startup Weekend | Sep'18 to Nov’19 \", 'Create Social Media Content for Facebook, Instagram, Twitter and LinkedIn.', 'Communicate with design and partnership teams to keep everything in order.', 'Communicate and interact with VCs, mentors, founders.', 'Communicate and interact with participants.', 'MC for the events.', 'Network and build connections.', '\\xa0', 'SEO FRIENDLY CONTENT CREATOR', 'Digital Edge | September’2016 to October’18 ', 'Create unique, attractive and SEO friendly content for E-Commerce Websites and', 'online blogs.', 'Research and analyse the necessary topics.', '', 'ONLINE TUTOR(English)', 'Sling | September’2016 to November’2016', 'Increase the spoken English proficiency of our app users through text and voice', 'messages.', '', 'POSITION OF LEADERSHIP', '', '• Literary Club Head | 2019-2020', '• Marketing and Sponsorship team Lead - Hackiton 3.0', \"• Event Co-ordinator for Cultura'19\", '', 'CERTIFICATIONS', 'Python Specialization: Coursera', '', 'Mathematics for Machine Learning', 'Specialization: Coursera', '', 'Machine Learning by Andrew Ng:', 'Coursera', '', 'TECHNICAL SKILLS', '', 'Tableau', ' Python', ' R', ' Java', ' MS-Excel', ' Machine Learning', ' Business Statistics', ' SQLlite', '', 'EDUCATION', '', '·\\xa0BE', 'CMR Institute of Technology', 'Stream-CSE', 'CGPA-7.47', '\\xa0', '· XII', 'AKLANK PUBLIC SCHOOL', 'CBSE- 79%', '\\xa0', '· X', 'DON BOSCO, PARK', 'CIRCUS', 'ICSE-89%', '', '\\x0c']\n"
          ]
        }
      ],
      "source": [
        "# text Extraction\n",
        "from pdfminer.high_level import extract_text\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)\n",
        "\n",
        "txt = extract_text_from_pdf(resume)\n",
        "print(txt)\n",
        "lst=txt.split('\\n')\n",
        "print(lst)\n",
        "res={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN2WpsKzZr8q"
      },
      "outputs": [],
      "source": [
        "import pdfx\n",
        "import re\n",
        "def get_urls(path):\n",
        "  pdf = pdfx.PDFx(path)\n",
        "  urls=pdf.get_references_as_dict()\n",
        "  urls=urls['url']\n",
        "  urls_extracted=[]\n",
        "  for u in urls:\n",
        "    if(re.search('.com',u)):\n",
        "      urls_extracted.append(str(u))\n",
        "  return urls_extracted\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('maxent_ne_chunker')\n",
        "# nltk.download('words')\n",
        "# nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YME4XyFEaVe"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnUGj2SaFX7s"
      },
      "outputs": [],
      "source": [
        "# extracting phone no\n",
        "import re\n",
        "import subprocess\n",
        " \n",
        "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
        "def doc_to_text_catdoc(file_path):\n",
        "    try:\n",
        "        process = subprocess.Popen(  # noqa: S607,S603\n",
        "            ['catdoc', '-w', file_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            universal_newlines=True,\n",
        "        )\n",
        "    except (\n",
        "        FileNotFoundError,\n",
        "        ValueError,\n",
        "        subprocess.TimeoutExpired,\n",
        "        subprocess.SubprocessError,\n",
        "    ) as err:\n",
        "        return (None, str(err))\n",
        "    else:\n",
        "        stdout, stderr = process.communicate()\n",
        " \n",
        "    return (stdout.strip(), stderr.strip())\n",
        " \n",
        " \n",
        "def extract_phone_number(resume_text):\n",
        "    phone = re.findall(PHONE_REG, resume_text)\n",
        " \n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "        if resume_text.find(number) > 0 and len(number) < 16:\n",
        "            return number\n",
        "    return None\n",
        "\n",
        "# phone_number = extract_phone_number(text)\n",
        "# res['phone_number']=extract_phone_number(text)\n",
        "# print(res)\n",
        "# print(phone_number)  # noqa: T001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OS-8skeEFokq"
      },
      "outputs": [],
      "source": [
        "#extracting emails\n",
        "import re\n",
        " \n",
        "from pdfminer.high_level import extract_text\n",
        " \n",
        "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
        " \n",
        " \n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)\n",
        " \n",
        " \n",
        "def extract_emails(resume_text):\n",
        "    return re.findall(EMAIL_REG, resume_text)\n",
        " \n",
        " \n",
        "\n",
        "# emails = extract_emails(text)\n",
        "# res['email']=extract_emails(text)\n",
        "# if emails:\n",
        "#   print(emails[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CZhem23HfPR"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# extracting job phrases \n",
        "def extract_job_phrases(nlp_text, noun_chunks, data):\n",
        "    \n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    skills = list(data.columns.values)\n",
        "    jobDomains = []\n",
        "    # check for one-grams\n",
        "    for token in tokens:\n",
        "        # print(token,'1_gram')\n",
        "        if token.lower() in skills:\n",
        "            jobDomains.append(token)\n",
        "\n",
        "    # check for bi-grams and tri-grams\n",
        "    for token in noun_chunks:\n",
        "        # print(token,'bi_gram')\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            jobDomains.append(token)\n",
        "    return [i.capitalize() for i in set([i.lower() for i in jobDomains])]\n",
        "\n",
        "\n",
        "# extracting college\n",
        "def extract_college(rtxt, noun_chunks, file):\n",
        "    data = pd.read_csv(file)\n",
        "    colleges = []\n",
        "    for college in data:\n",
        "        if college in rtxt:\n",
        "            colleges.append(college)\n",
        "\n",
        "    return colleges\n",
        "\n",
        "\n",
        "#extracting skills\n",
        "def extract_skills(nlp_text, noun_chunks, file):\n",
        "    \n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    \n",
        "    data = pd.read_csv(file)\n",
        "    skills = list(data.columns.values)\n",
        "    skillset = []\n",
        "    \n",
        "    # check for one-grams\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            skillset.append(token)\n",
        "\n",
        "    # check for bi-grams and tri-grams\n",
        "    for token in noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            skillset.append(token)\n",
        "\n",
        "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mKiHI2mJ6w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ea0abe-9d83-4f04-80b8-038cca459129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.24.3) or chardet (4.0.0) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "def extract_noun_chunks(text):\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  # text = \"\"\n",
        "  # for t in text_raw:\n",
        "  #   text += t\n",
        "\n",
        "  nlp = nlp(text)\n",
        "  noun_chunks = list(nlp.noun_chunks)\n",
        "  return nlp, noun_chunks\n",
        "\n",
        "# skills = extract_skills(nlp,noun_chunks,'skills.csv')\n",
        "# res['skills']=extract_skills(nlp,noun_chunks,'skills.csv')\n",
        "# print(skills)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoSkTvjnRcMu"
      },
      "outputs": [],
      "source": [
        "# extracting Job Phrases\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text_raw = extract_text_from_pdf(resume)\n",
        "text = \"\"\n",
        "\n",
        "for t in text_raw:\n",
        "  text += t\n",
        "\n",
        "nlp = nlp(text)\n",
        "noun_chunks = list(nlp.noun_chunks)\n",
        "\n",
        "data = pd.read_csv('jobphrases.csv')\n",
        "jp = extract_job_phrases(nlp,noun_chunks,data)\n",
        "# res['roles']=extract_job_phrases(nlp,noun_chunks,data)\n",
        "\n",
        "# print(noun_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPDoiVKr1Rle"
      },
      "outputs": [],
      "source": [
        "# # extracting college name, degree and CGPA\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "# text_raw = extract_text_from_pdf(resume)\n",
        "# text = \"\"\n",
        "# l = []\n",
        "# for t in text_raw:\n",
        "#   text += t\n",
        "\n",
        "# college = extract_college(text,noun_chunks, 'colleges2.csv')\n",
        "# res['college'] = college\n",
        "# print(college)\n",
        "import re\n",
        "def get_education_details(text, noun_chunks):\n",
        "  college = extract_college(text,noun_chunks, 'colleges2.csv')\n",
        "  degree = extract_college(text,noun_chunks, 'allmajors.csv')\n",
        "  gpa= []\n",
        "  for c in college:\n",
        "    idx = text.index(c)\n",
        "    a = text[idx-20:idx+200] #the reduced text space\n",
        "    \n",
        "    \n",
        "    set1 = {\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"}\n",
        "    for i in range(len(a)):\n",
        "      if a[i] == \".\" and a[i+1] in set1 and a[i-1] in set1 and a[i+2] != \"%\" and a[i+3]!=\"%\":\n",
        "        gpacopy = (a[i-1]+a[i]+a[i+1]+a[i+2]+\" CGPA\")\n",
        "        if gpacopy not in gpa:\n",
        "          gpa.append(gpacopy)\n",
        "\n",
        "  x = re.findall(r\"(\\d+(\\.\\d+)?%)\", text)\n",
        "  x = [j for j,i in x]\n",
        " \n",
        "  return college, degree, gpa, list(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnjBhqN68hCu",
        "outputId": "f0b13675-724c-4ae1-acc7-cbe0239bb735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"Zeeshan Islam\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !pip install pytorch-pretrained-bert==0.4.0\n",
        "# import torch\n",
        "# model_save_name = 'name_ner.pt'\n",
        "# path = F\"/content/drive/My Drive/{model_save_name}\" \n",
        "# mq=torch.load(path)\n",
        "# mq.eval()\n",
        "res['name']='Zeeshan Islam'\n",
        "# se=res['roles']\n",
        "# se.remove('E')\n",
        "# print(se)\n",
        "import json\n",
        "f_res = json.dumps(res,indent=2)\n",
        "print(f_res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VSjdZUHFKeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c88efa69-90b3-4a7c-fe95-475072f15649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Grad all general stop words\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "EDUCATION = [\n",
        "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
        "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
        "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
        "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
        "        ]\n",
        "\n",
        "def extract_education(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # Sentence Tokenizer\n",
        "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
        "\n",
        "    edu = {}\n",
        "    # Extract education degree\n",
        "    for index, text in enumerate(nlp_text):\n",
        "        for tex in text.split():\n",
        "            # Replace all special symbols\n",
        "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
        "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
        "                edu[tex] = text + nlp_text[index + 1]\n",
        "\n",
        "    # Extract year\n",
        "    education = []\n",
        "    for key in edu.keys():\n",
        "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
        "        if year:\n",
        "            education.append((key, ''.join(year[0])))\n",
        "        else:\n",
        "            education.append(key)\n",
        "    return education"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugjpOTgwH0eE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0812e5d6-5f11-435e-c582-c6266dae7421"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BE', 'XII', 'X']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "extract_education(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-xw0D9sl2nf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files"
      ],
      "metadata": {
        "id": "Le5nlTOA2pZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "id": "fuHG1DUc3RSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import glob\n",
        "# files = glob.glob(\"*.pdf\")\n",
        "# print(files)"
      ],
      "metadata": {
        "id": "u1DmJ21a4uEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I5WNPuIo6vFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tika"
      ],
      "metadata": {
        "id": "3RMnEU726xk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from tika import parser  \n",
        "\n",
        "# def extract_text_tika(path):  \n",
        "#   # opening pdf file\n",
        "#   parsed_pdf = parser.from_file(path)\n",
        "    \n",
        "#   # saving content of pdf\n",
        "#   # you can also bring text only, by parsed_pdf['text'] \n",
        "#   # parsed_pdf['content'] returns string \n",
        "#   data = parsed_pdf['content']\n",
        "\n",
        "#   return data \n",
        "\n"
      ],
      "metadata": {
        "id": "hpH1_VIe847W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def get_details(text, path, noun_chunks, nlp):\n",
        "    res={}\n",
        "    res['phone_number']=extract_phone_number(text)\n",
        "    res['email']=extract_emails(text)\n",
        "    res['skills']=extract_skills(nlp,noun_chunks,'skills.csv')\n",
        "    res['roles']=extract_job_phrases(nlp,noun_chunks,data)\n",
        "    res[\"college\"], res[\"course\"], res[\"gpa\"], res['percentages'] = get_education_details(text, noun_chunks)\n",
        "    \n",
        "\n",
        "    # res['public_profiles']=get_urls(resume)\n",
        "    f_res = json.dumps(res,indent=2)\n",
        "    return f_res"
      ],
      "metadata": {
        "id": "h7PE1nwt-78N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "txt = extract_text_from_pdf(resume)\n",
        "text = ''\n",
        "for t in txt:\n",
        "  text += t\n",
        "# print(text)\n",
        "noun_chunks_text, noun_chunks_list = extract_noun_chunks(txt)\n",
        "print(get_details(text, resume, noun_chunks_list, noun_chunks_text))\n",
        "print(\"\\n\\n---------------------------------------------------------------------------------\\n\\n\")\n"
      ],
      "metadata": {
        "id": "9ke9ehHlBbOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c9cf10-227f-44f0-cb65-9f08bd2411ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"phone_number\": \"9830450347\",\n",
            "  \"email\": [\n",
            "    \"zeeshanislam1511998@gmail.com\",\n",
            "    \"zees17is@cmrit.ac.in\"\n",
            "  ],\n",
            "  \"skills\": [\n",
            "    \"Excel\",\n",
            "    \"Content\",\n",
            "    \"Technical\",\n",
            "    \"Twitter\",\n",
            "    \"Facebook\",\n",
            "    \"Statistics\",\n",
            "    \"Tableau\",\n",
            "    \"Marketing\",\n",
            "    \"Mobile\",\n",
            "    \"Python\",\n",
            "    \"Seo\",\n",
            "    \"Startup\",\n",
            "    \"Research\",\n",
            "    \"English\",\n",
            "    \"R\",\n",
            "    \"Machine learning\",\n",
            "    \"Partnership\",\n",
            "    \"Design\",\n",
            "    \"Java\",\n",
            "    \"Mathematics\"\n",
            "  ],\n",
            "  \"roles\": [\n",
            "    \"Analyst\",\n",
            "    \"Intern\",\n",
            "    \"E\",\n",
            "    \"Data analyst intern\"\n",
            "  ],\n",
            "  \"college\": [\n",
            "    \"CMR Institute of Technology\"\n",
            "  ],\n",
            "  \"course\": [\n",
            "    \"CSE\"\n",
            "  ],\n",
            "  \"gpa\": [\n",
            "    \"7.47 CGPA\"\n",
            "  ],\n",
            "  \"percentages\": [\n",
            "    \"79%\",\n",
            "    \"89%\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "jjyCY2U5BXDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3L8juJPVCL5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VN3MfOwXFvVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ResumeParser.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}